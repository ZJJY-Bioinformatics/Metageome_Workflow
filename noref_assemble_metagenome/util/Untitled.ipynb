{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c925be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pysam\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736e3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤长度小于300bp的orf\n",
    "for path in list(Path(\"05.predict/prodigal/\").glob(\"*.ffn\")):\n",
    "    out_file = open(str(path).replace(\"ffn\",\"300bp.fa\"),\"w\")\n",
    "    with pysam.FastxFile(str(path)) as fa:\n",
    "        for seq in fa:\n",
    "            if len(seq.sequence) >= 300:\n",
    "                out_file.write(\"seq.name\" + \" \" + seq.comment + \"\\n\" + seq.sequence + \"\\n\")\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3bae9a09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 合并GTDB的序列，并修改名字好\n",
    "import os\n",
    "import pysam\n",
    "from pathlib import Path\n",
    "\n",
    "out_file = open(\"/data3/wangjiaxuan/refer/GTDBTk_db_65703sp.fa\",\"w\")\n",
    "with open(\"/data3/wangjiaxuan/refer/metapi_db/GTDBTk_db/release207_v2/taxonomy/gtdb_taxonomy_test.tsv\",\"r\") as tax_list:\n",
    "    for line in tax_list:\n",
    "        asid = line.split(\"\\t\")[0].replace(\".1\",\"\")\n",
    "        asid = asid.split(\"_\")\n",
    "        asid_number = [asid[2][i*3:i*3+3] for i in range(3)]\n",
    "        path1 = os.path.join(\"/data3/wangjiaxuan/refer/metapi_db/GTDBTk_db/release207_v2/fastani/database\",\n",
    "                           asid[1],\n",
    "                           asid_number[0],\n",
    "                           asid_number[1],\n",
    "                           asid_number[2])\n",
    "        fa_file = Path(path1).glob(\"*.gz\")\n",
    "        with pysam.FastxFile(str(list(fa_file)[0])) as fa:\n",
    "            for seq in fa:\n",
    "                #print(\"_\".join([asid[0],asid[1]]) + \"_\" + \"\".join(asid_number)+ \".1\" + \" \" + seq.name + \" \" + seq.comment + \"\\n\")\n",
    "                out_file.write(\">\"+\"_\".join([asid[0],asid[1]]) + \"_\" + \"\".join(asid_number)+ \".1\" + \" \" + seq.name + \" \" + seq.comment + \"\\n\" + seq.sequence + \"\\n\")\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76cba097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并samtools的统计结果\n",
    "\n",
    "# function\n",
    "\n",
    "import os\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from decimal import *\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "\n",
    "def flagstats_summary(flagstats, method = 2, **kwargs):\n",
    "    \"\"\"\n",
    "    get alignment rate from sorted bam file\n",
    "    samtools flagstat --threads 8 sample.sort.bam\n",
    "    \"\"\"\n",
    "    mapping_info = []\n",
    "    getcontext().prec = 8\n",
    "\n",
    "    # with open(flagstat_list, 'r') as list_handle:\n",
    "    if method == 1:\n",
    "        list_handle = open(flagstats, \"r\")\n",
    "    if method == 2:\n",
    "        list_handle = flagstats\n",
    "\n",
    "    for flagstat_file in list_handle:\n",
    "        if os.path.exists(flagstat_file.strip()):\n",
    "            info = {}\n",
    "            info[\"sample_id\"] = os.path.basename(flagstat_file.strip()).split(\".\")[0]\n",
    "            stat_list = open(flagstat_file.strip(), \"r\").readlines()\n",
    "\n",
    "            info[\"total_num\"] = stat_list[0].split(\" \")[0]\n",
    "\n",
    "            if len(stat_list) == 13:\n",
    "                info[\"read_1_num\"] = stat_list[6].split(\" \")[0]\n",
    "                info[\"read_2_num\"] = stat_list[7].split(\" \")[0]\n",
    "\n",
    "                mapped = re.split(r\"\\(|\\s+\", stat_list[4])\n",
    "                info[\"mapped_num\"] = mapped[0]\n",
    "                info[\"mapped_rate\"] = Decimal(mapped[5].rstrip(\"%\")) / Decimal(100)\n",
    "            \n",
    "                #primary_mapped = re.split(r\"\\(|\\s+\", stat_list[5])\n",
    "                #info[\"primary_mapped_num\"] = primary_mapped[0]\n",
    "                #info[\"primary_mapped_rate\"] = Decimal(primary_mapped[6].rstrip(\"%\")) / Decimal(100)\n",
    "\n",
    "            elif len(stat_list) == 16:\n",
    "                info[\"read_1_num\"] = stat_list[9].split(\" \")[0]\n",
    "                info[\"read_2_num\"] = stat_list[10].split(\" \")[0]\n",
    "\n",
    "                mapped = re.split(r\"\\(|\\s+\", stat_list[6])\n",
    "                info[\"mapped_num\"] = mapped[0]\n",
    "                info[\"mapped_rate\"] = Decimal(mapped[5].rstrip(\"%\")) / Decimal(100)\n",
    "            \n",
    "                primary_mapped = re.split(r\"\\(|\\s+\", stat_list[7])\n",
    "                info[\"primary_mapped_num\"] = primary_mapped[0]\n",
    "                info[\"primary_mapped_rate\"] = Decimal(primary_mapped[6].rstrip(\"%\")) / Decimal(100)\n",
    " \n",
    "            paired = re.split(r\"\\(|\\s+\", stat_list[-5])\n",
    "            info[\"paired_num\"] = paired[0]\n",
    "            paired_rate = paired[6].rstrip(\"%\")\n",
    "            if paired_rate != \"N/A\":\n",
    "                info[\"paired_rate\"] = Decimal(paired_rate) / Decimal(100)\n",
    "                info[\"mapping_type\"] = \"paired-end\"\n",
    "            else:\n",
    "                info[\"paired_rate\"] = paired_rate\n",
    "                info[\"mapping_type\"] = \"single-end\"\n",
    "\n",
    "            singletons = re.split(r\"\\(|\\s+\", stat_list[-3])\n",
    "            info[\"singletons_num\"] = singletons[0]\n",
    "            singletons_rate = singletons[5].rstrip(\"%\")\n",
    "            if singletons_rate != \"N/A\":\n",
    "                info[\"singletons_rate\"] = Decimal(singletons_rate) / Decimal(100)\n",
    "            else:\n",
    "                info[\"singletons_rate\"] = singletons_rate\n",
    "\n",
    "            info[\"mate_mapped_num\"] = re.split(r\"\\(|\\s+\", stat_list[-2])[0]\n",
    "            info[\"mate_mapped_num_mapQge5\"] = re.split(r\"\\(|\\s+\", stat_list[-1])[0]\n",
    "            mapping_info.append(info)\n",
    "\n",
    "    mapping_info_df = pd.DataFrame(mapping_info)\n",
    "    if \"output\" in kwargs:\n",
    "        mapping_info_df.to_csv(kwargs[\"output\"], sep=\"\\t\", index=False)\n",
    "    return mapping_info_df\n",
    "\n",
    "in_file_list = [str(x) for x in list(Path(\"../02.rmhost\").glob(\"*.align2host.flagstat\"))]\n",
    "\n",
    "sum_flagstat= flagstats_summary(in_file_list)\n",
    "\n",
    "sum_flagstat.to_csv(\"../02.rmhost/mapping_host_stat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27873bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the bam flagstat\n",
    "trim_fastp_stat = open(\"01.fastq_qc/fastq_trim_stats.txt\",\"w\")\n",
    "with open(\"01.fastq_qc/fastp_multiqc_report_data/multiqc_general_stats.txt\",\"r\") as f:\n",
    "    i = 1\n",
    "    for line in f:\n",
    "        if i == 1:\n",
    "            l = line.replace(\"fastp_mqc-generalstats-fastp-\",\"\").title()\n",
    "        else:\n",
    "            l = line\n",
    "        trim_fastp_stat.write(l)\n",
    "        i += 1\n",
    "trim_fastp_stat.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5181a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge fastp QC result\n",
    "\n",
    "import os\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse(stats_file,sep = \"\\t\"):\n",
    "    if os.path.exists(stats_file):\n",
    "        try:\n",
    "            df = pd.read_csv(stats_file, sep = sep)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"%s is empty, please check\" % stats_file)\n",
    "            return None\n",
    "\n",
    "        if not df.empty:\n",
    "            return df\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        print(\"%s is not exists\" % stats_file)\n",
    "        return None\n",
    "\n",
    "\n",
    "def merge(input_list, func, workers = 4, **kwargs):\n",
    "    df_list = []\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "        for df in executor.map(func, input_list):\n",
    "            if df is not None:\n",
    "                df_list.append(df)\n",
    "\n",
    "    df_ = pd.concat(df_list)\n",
    "\n",
    "    if \"output\" in kwargs:\n",
    "        df_.to_csv(kwargs[\"output\"], sep=\"\\t\", index=False)\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f6b39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01.fastq_qc\n",
    "fastp_file_list = [x for x in list(Path(\"01.fastq_qc/\").glob(\"*_trimming_stats.tsv\"))]\n",
    "fastp_stat = merge(fastp_file_list,parse,8)\n",
    "fastp_stat.to_csv(\"01.fastq_qc//trimming_fastp_stat.csv\")\n",
    "# 02.rmhost\n",
    "fastp_file_list = [x for x in list(Path(\"02.rmhost/\").glob(\"*_rmhost_stats.tsv.raw\"))]\n",
    "fastp_stat = merge(fastp_file_list,parse,8)\n",
    "fastp_stat.to_csv(\"02.rmhost/rmhost_fastp_stat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb2dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "89c19913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载\n",
    "import re\n",
    "from collections import defaultdict\n",
    "'''\n",
    "这是为了将输入的总表，按照分类级别输出表格\n",
    "'''\n",
    "# 词典\n",
    "taxonomy_level = [\n",
    "    \"domain\",\n",
    "    \"superkingdom\",\n",
    "    \"phylum\",\n",
    "    \"class\",\n",
    "    \"order\",\n",
    "    \"family\",\n",
    "    \"genus\",\n",
    "    \"species\",\n",
    "    \"strain\",\n",
    "]\n",
    "\n",
    "taxonomy_dict = {\n",
    "    \"strain\": r\"t__(.*?)[\\|\\t]\",\n",
    "    \"species\": r\"s__(.*?)[\\|\\t]\",\n",
    "    \"genus\": r\"g__(.*?)[\\|\\t]\",\n",
    "    \"family\": r\"f__(.*?)[\\|\\t]\",\n",
    "    \"order\": r\"o__(.*?)[\\|\\t]\",\n",
    "    \"class\": r\"c__(.*?)[\\|\\t]\",\n",
    "    \"phylum\": r\"p__(.*?)[\\|\\t]\",\n",
    "    \"superkingdom\": r\"k__(.*?)[\\|\\t]\",\n",
    "    \"domain\" : r\"d__(.*?)[\\|\\t]\",\n",
    "}\n",
    "\n",
    "level_dict = {\n",
    "    \"strain\": \"t\",\n",
    "    \"species\": \"s\",\n",
    "    \"genus\": \"g\",\n",
    "    \"family\": \"f\",\n",
    "    \"order\": \"o\",\n",
    "    \"class\": \"c\",\n",
    "    \"phylum\": \"p\",\n",
    "    \"superkingdom\": \"k\",\n",
    "    \"domain\" : \"d\"\n",
    "}\n",
    "\n",
    "# 输入\n",
    "taxonomy_table = open(\"test.tsv\",\"r\")\n",
    "# 输出\n",
    "level_output_file = defaultdict(list)\n",
    "level_output_header = dict()\n",
    "\n",
    "for line in taxonomy_table:\n",
    "    # 拆分分类和表达量\n",
    "    taxonomy_info = line.split(\"\\t\")[0]\n",
    "    exp_info = line.split(\"\\t\")[1:]\n",
    "    taxonomy_name = []\n",
    "    # header的输出\n",
    "    if \"clade_name\" in line and \"#\" not in line:\n",
    "        level_output_header['header'] = \"\\t\".join(taxonomy_level)+\"\\t\"+\"\\t\".join(exp_info)\n",
    "    elif \"#\" not in line:\n",
    "        # 名字输出\n",
    "        final_taxonomy_var = \"d__unclassified\"\n",
    "        for lev in taxonomy_level:\n",
    "            re_result = re.findall(taxonomy_dict[lev],line)\n",
    "            if re_result:\n",
    "                taxonomy_name.append(level_dict[lev]+\"__\"+re_result[0])\n",
    "                final_taxonomy_var = lev\n",
    "            else:\n",
    "                taxonomy_name.append(level_dict[lev]+\"__\"+\"unclassified\")\n",
    "        level_output_file['all'].append(\"\\t\".join(taxonomy_name)+\"\\t\"+\"\\t\".join(exp_info))\n",
    "        level_output_file[final_taxonomy_var].append(\"\\t\".join(taxonomy_name)+\"\\t\"+\"\\t\".join(exp_info))\n",
    "\n",
    "for lev in level_output_file:\n",
    "    with open(f'03.taxonomy/all_sample_taxonomy_profile_{lev}.xls',\"w\") as f:\n",
    "        f.write(level_output_header['header'])\n",
    "        for line in level_output_file[lev]:\n",
    "            f.write(line)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
